---
language:
- en
license: mit
library_name: mobilenetv2
tags:
- CNN
- image-classification
- architectural-recognizion
metrics:
- accuracy
---

# Model Card for the "Architectural Style Recognition CNN"

## Table of Contents
- [Model details](#model-details)
- [Parameters](#parameters)
- [Resources](#resources)
	- [Related Works](#related-works)
- [License](#license)
- [Contacts](#contacts)
- [Intended use](#intended-use)
- [Factors](#factors)
- [Metrics](#metrics)
- [Training Data](#training-data)
	- [Motivation](#motivation)
	- [Preprocessing](#preprocessing)
- [Caveats and recommendations](#caveats-and-recommendations)


### Model details

- Authors: Andrea Basile, Roberto Lorusso, Antonio Iacovazzi, Emanuele Pomponio

- Model date: 28 June 2022

- Model Version: 2.2.3

- Model Type: CNN based on Mobile-NetV2 architecture 


The model we used in our project is composed by 4 Layers:

- Input layer: used for establish the shape of input, in this case an image 224 x 224 x 3

- MobileNetV2: MobileNet is a pretrained CNN, suited for devices with low computational resources. We retrained all the parameters of this network in order to get better accuracy.

- Dropout layer: randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.

- Dense layer: the dense layer performs a matrix- vector multiplication. The output generated by the dense layer is an 10 dimensional vector in order to obtain the classification through the ac- tivation function "softmax".


### Parameters 

The model was trained with the following setting for the parameters:
-  Stochastic Gradient Descent optimizer with learning rate of 0.005 and momentum of 0.9
-  Number of epochs = 10
-  Batch size = 32
-  steps per epoch = train set size / batch size
- validation steps = validation set size / batch size
- Dropout rate = 0.4
- L2 regularizer = 0.005



### Resources

The dataset is publicly available from kaggle at:
<a>https://www.kaggle.com/datasets/wwymak/architecture-dataset</a>

#### Related Works: 

- Zhe Xu et al. “Architectural Style Classification Using Multinomial Latent Logistic Regression”. In: ECCV. 2014.

- Zhang J. et al Wang B. Zhang S. “Architectural style classification based on CNN and channel spatial attention”. In: SIViP. 2022.

- Mark Sandler et al. “Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation”. In: CoRR abs/1801.04381 (2018). arXiv: 1801.04381. url: <a>http://arxiv.org/abs/1801.04381 </a>

- Chi-Feng Wang. A Basic Introduction to Separable Convolutions. 2018 url: <a>https://towardsdatascience.com/ a-basic-introduction-to-separable-convolutions-b99ec3102728</a>


### License 

MIT License

### Contacts

Further details can be given at the following contacts:   

- a.basile99@studenti.uniba.it
- r.lorusso62@studenti.uniba.it


### Intended use

The principal stakeholders are generic users with the intent of recognizing an architectural style. The model has a user-interface developed with a Telegram chat bot. 
Another context of usage can be the didactic one, as every organization can access to the chat bot. 

### Factors

The training and validatoin phases are made by using only images captured at daylight time. In the test phase there could be low accuracy associated to night images.

All the experiments were carried out on Google Colab platform with the following setting:

- Python 2.7
- TensorFlow 2.8.0
- GPU Hardware Accelerator


### Metrics

The accuracy obtained by the model in the training phase is around 95%, while the validation accuracy is around the 84%. 

During the test phase, the model reached an accuracy of 84% on a test set of 450 images. 


### Training data 

The training data consists of 80% of our dataset, the remaining 20% is divided between validation data and test data.

#### Motivation 

The main motivation behind the choice of the dataset is that it is the largest publicly available data set for architectural style classification.

#### Preprocessing

Since the dataset was highly unbalanced, we decided to perform both data balancing and data augmentation. The goals of these decisions are: 

- Equally represented classes with 450 images each,
- Mapping features of the classes regardless of the eventual presence of noise in the images. 

Data balancing is performed by duplicating images in the classes. 
Data augmentation is performed by appliying the following transformations on the training images: 

- Re-scaling normalization: Neural networks pro- cess inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. As such it is good practice to normalize the pixel values so that each pixel value has a value between 0 and 1.

- Mean-STD Normalization: Data normalization is an important step in the training process of a neural network. By normalizing the data to a uniform mean of 0 and a standard deviation of 1, faster convergence is achieved.

- Random-rotation

- Random-scaling

- Random-zoom


### Caveats and Recommendations

Since the model was trained on a limited number of architectural styles, the predictions can be wrong when the model tries to classify unseen architectural styles, classifying the image with the most similar one. 
The informations gained after its use must be verified by trustful sources. 

Since architectural styles can enclose many other styles, the predicitons can be sensitive to this kind of characteristics.

As mentioned before night images can suffer of low accuracy in the process of prediction, we suggest to run predictions on images captured at day light time. 